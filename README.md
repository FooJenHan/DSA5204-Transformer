# Transformer
DSA5204 Project
Empirical investigation of gradient explosion problem in encoder-decoder neural nets. 
Simple RNNs, GRUs, LSTMs, each endowed with attention layers, are compared against attention only neural nets (the Transformer). Weights of each neural net are tracked and compared over 100 epochs for signs of gradient explosion. 
All neural nets are trained on the same Sequence to sequence problem; Ted Open Translation Project, Portuguese-to-English dataset.
